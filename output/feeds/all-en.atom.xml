<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Another Life Form</title><link href="http://mfactor.sdf.org/" rel="alternate"></link><link href="http://mfactor.sdf.org/feeds/all-en.atom.xml" rel="self"></link><id>http://mfactor.sdf.org/</id><updated>2016-01-16T21:07:00-02:00</updated><entry><title>Data Science workflow with reproducible research - Jupyter notebooks</title><link href="http://mfactor.sdf.org/data-science-workflow-with-reproducible-research-jupyter-notebooks.html" rel="alternate"></link><updated>2016-01-16T21:07:00-02:00</updated><author><name>Ivan Marin</name></author><id>tag:mfactor.sdf.org,2016-01-16:data-science-workflow-with-reproducible-research-jupyter-notebooks.html</id><summary type="html">&lt;p&gt;As a follow up of the &lt;a href="http://mfactor.sdf.org/data-science-workflow-with-reproducible-research.html"&gt;previous post&lt;/a&gt;, here I present a &lt;a href="https://jupyter.org"&gt;Jupyter&lt;/a&gt; notebook that incorporates a few
things that makes easier to follow the ideias of the data project structure. &lt;/p&gt;
&lt;p&gt;The notebook can be found &lt;a href="https://github.com/ispmarin/ds_workshop/blob/master/src/template_ds_workshop.ipynb"&gt;here&lt;/a&gt;. 
Using the idea of auto documentation of the notebooks, this notebook details the ideas. &lt;/p&gt;</summary><category term="data science"></category><category term="data"></category></entry><entry><title>Data Science workflow with reproducible research</title><link href="http://mfactor.sdf.org/data-science-workflow-with-reproducible-research.html" rel="alternate"></link><updated>2015-12-08T22:42:00-02:00</updated><author><name>Ivan Marin</name></author><id>tag:mfactor.sdf.org,2015-12-08:data-science-workflow-with-reproducible-research.html</id><summary type="html">&lt;h1&gt;Data science workflow&lt;/h1&gt;
&lt;p&gt;I've been wondering and tinkering about how to structure my data science projects, thinking on the lines of &lt;a href="http://reproducibleresearch.net/"&gt;Reproducible Research&lt;/a&gt; and separation of information. There is a lot of talk about the CRISP-DM &lt;a href="https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining"&gt;Cross Industry Standard Process for Data Mining&lt;/a&gt; and I believe that CRISP-DM is indeed a good approach. There are others like &lt;a href="https://en.wikipedia.org/wiki/SEMMA"&gt;SEMMA&lt;/a&gt; that are similar. But how these processes come down to a real project organization? Breaking down the steps, in a general overview, a data science project generally has&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;, either the datasets or samples, that will be worked on&lt;/li&gt;
&lt;li&gt;&lt;code&gt;src&lt;/code&gt; (a code folder), for analysis, ETL and modeling&lt;/li&gt;
&lt;li&gt;&lt;code&gt;documentation&lt;/code&gt; about the problem, business case, the data and the solution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;results&lt;/code&gt;, in form of processed data, a software deployment process or final documentation (including presentation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course your project will have other needs or either a more complex or more simple structure, also depending on how you think about CRISP-DM or your own approach. But so far this structure above has helped me to keep things organized and understandable based on CRISP-DM. I try to keep in line the ideas from both the process and reproducible research.&lt;/p&gt;
&lt;p&gt;I also have a few other requirements for my projects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;all code must be versioned&lt;/li&gt;
&lt;li&gt;all data must be identified by data and local&lt;/li&gt;
&lt;li&gt;the documentation should also be versioned a preferably autogenerated&lt;/li&gt;
&lt;li&gt;the results should be reproducible, without intervention&lt;/li&gt;
&lt;li&gt;libraries should be isolated from the system and preferably portable to other systems&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Requirements 1 and 3 are satisfied using &lt;code&gt;git&lt;/code&gt;. I usually set the root of the project as the &lt;code&gt;git&lt;/code&gt; base directory and exclude on the &lt;code&gt;.gitignore&lt;/code&gt; the data and results directories. I decided not to include the &lt;code&gt;results&lt;/code&gt; folder because of requirement 4 and also because they can be big. An exception can be made if there are presentations inside &lt;code&gt;results&lt;/code&gt; that are not autogenerated. Part of the autogeneration of the documentation is made writing it on the analysis code itself, a la &lt;code&gt;jupyter&lt;/code&gt; notebook style.&lt;/p&gt;
&lt;p&gt;Requirement 2 is satisfied using a simple directory structure. Inside the &lt;code&gt;data&lt;/code&gt; folder I create a directory with the date as the name, like &lt;code&gt;20151207&lt;/code&gt; and include hour and minute if I'm generating samples with this precision. If I need to generate a sample a &lt;code&gt;sample&lt;/code&gt; directory inside the dated directory suffices. Some people advise to save intermediate steps if there are a lot of data cleanup steps. I tend to do it if the cleanup step is too time consuming, otherwise I just recompute the cleanup step. I also tend to include the metadata with the data, outside the data file, with the same name as the data file but with a &lt;code&gt;.meta&lt;/code&gt; termination.  &lt;/p&gt;
&lt;p&gt;The local part of the &lt;code&gt;data&lt;/code&gt; can be tricky when using a database as a source, for example. This is one of the points where reproducible research helps a bit: databases can change, so let's say you run today a model with data pulled from a database. You should be able to reproduce exactly the same results any time, but the database could have been updated in the meantime and your project is not reproducible anymore. I download the data to a local storage file (either csv or HDF5 or other format at hand) and use this file to do my computations. There is a real problem if the data set is too large or the data is sensitive, but so far I've managed to do it without serious issues. &lt;/p&gt;
&lt;p&gt;I usually use &lt;code&gt;jupyter&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; to do my analysis and modeling, so to satisfy requirement 4 I tend to write my notebooks and code so they are idempotent and can be restarted from different points. That also means that the results would be overwritten everytime the code is run. This can be a problem if a experiment is being done with different inputs, so this should be taken into account when thinking on the structure of the &lt;code&gt;results&lt;/code&gt; folder (It can use the same dated approach from the &lt;code&gt;data&lt;/code&gt; directory).&lt;/p&gt;
&lt;p&gt;The last requirement depends on the language or languages that are being used in the project. This is a rather lengthy subject, so I will restrict myself to Python. I use &lt;code&gt;virtualenvs&lt;/code&gt; without site packages and keep them on the &lt;code&gt;src&lt;/code&gt; folder. Each project has its own virtualenv. Other requirements, like map data, is also included on the project structure, so the entire project folder can be moved without great impact. Virtualenvs are not that portable between systems and this has been a problem so far without solution, especially if the other system doesn't have internet access. &lt;/p&gt;
&lt;h2&gt;Project Example&lt;/h2&gt;
&lt;p&gt;As an example I have the &lt;code&gt;polls&lt;/code&gt; project, with the following directory structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="na"&gt;.pools&lt;/span&gt;
&lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;data&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="err"&gt;20151206&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls.csv&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls.meta&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;
         &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls_sample_1000.csv&lt;/span&gt;
         &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls_sample_200.csv&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="err"&gt;20151203&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls_rs.csv&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;
         &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls_sample_rs_1000.csv&lt;/span&gt;
         &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;polls_sample_pe_200.csv&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="err"&gt;20151202&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;all_polls.hdf&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;all_polls.meta&lt;/span&gt;
&lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;src&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;analysis&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;pools.ipynb&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;sampling&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;sampling.py&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;etl&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;extract_from_mongo.py&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;generate_results&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;do_models.py&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;generate_webpage.py&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;generate_approach_action.py&lt;/span&gt;
         &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;images&lt;/span&gt;
            &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;header.png&lt;/span&gt;
            &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;logo.svg&lt;/span&gt;
&lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;doc&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;business&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;use_case.md&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;algorithm&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;machine_learning.md&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;analysis_results.md&lt;/span&gt;
         &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;images&lt;/span&gt;
            &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;analysis_all.png&lt;/span&gt;
&lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;results&lt;/span&gt;
   &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="err"&gt;20151207&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;voters.csv&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;control_group.csv&lt;/span&gt;
      &lt;span class="err"&gt;+--&lt;/span&gt; &lt;span class="nf"&gt;approach_and_action.md&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One of the things that I also consider is the use of symbolic links from the &lt;code&gt;src&lt;/code&gt; folder to the &lt;code&gt;results&lt;/code&gt; folder or do a copy of a figure, from example. &lt;/p&gt;
&lt;p&gt;On the next article I plan to show a pattern for &lt;code&gt;jupyter&lt;/code&gt; notebooks to handle this structure. &lt;/p&gt;</summary><category term="data science"></category><category term="data"></category></entry><entry><title>Using `conda` instead of `pip`</title><link href="http://mfactor.sdf.org/conda.html" rel="alternate"></link><updated>2015-11-26T22:42:00-02:00</updated><author><name>Ivan Marin</name></author><id>tag:mfactor.sdf.org,2015-11-26:conda.html</id><summary type="html">&lt;p&gt;I've been struggling with which environment use to develop on Python applications that 
are going to be moved around and run on different systems. There is Hadoop and Spark on the mix, so things are getting
more complicated, too. Thinking about my environment, I have the following requisites:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the level of isolation of the application.&lt;/li&gt;
&lt;li&gt;Point to the &lt;code&gt;python&lt;/code&gt; binary that I want.&lt;/li&gt;
&lt;li&gt;Can be moved around &lt;/li&gt;
&lt;li&gt;Can be deployed and used in systems without internet&lt;/li&gt;
&lt;li&gt;Can be deployed and used without root access&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So far I have two alternatives: &lt;code&gt;virtualenv&lt;/code&gt; and &lt;code&gt;Anaconda&lt;/code&gt;. I've been using &lt;code&gt;virtualenv&lt;/code&gt; for a while so I know 
better the limitations. I tried, then, to use &lt;code&gt;Anaconda&lt;/code&gt; to see how it works. The installer is heavy but it comes
with several useful things (&lt;code&gt;pandas&lt;/code&gt;, for example), so that's ok. Things got a bit complicated using the environments,
though, with the &lt;code&gt;conda&lt;/code&gt; application. To keep point 2 I changed the recommended &lt;code&gt;PATH&lt;/code&gt; variable to &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;export PATH=&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="x"&gt;:/home/ispmarin/bin:/home/ispmarin/bin/anaconda3/bin&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;so I get the system &lt;code&gt;python&lt;/code&gt; executable but all the other tools from &lt;code&gt;Anaconda&lt;/code&gt;. Nice. Then I created and environment
and installed some stuff there. Also works as expected, although it points to the &lt;code&gt;python&lt;/code&gt; binary from inside &lt;code&gt;Anaconda&lt;/code&gt;.
Acceptable. Then I deactivated the environment, and to my surprise, the &lt;code&gt;python&lt;/code&gt; executable didn't change back to 
the system one! Damn, that's a problem right there. There 
&lt;a href="http://stackoverflow.com/questions/33955516/anaconda-activate-deactivate-cycle-messes-up-path"&gt;are&lt;/a&gt; 
&lt;a href="https://github.com/conda/conda/issues/1846"&gt;several&lt;/a&gt; 
&lt;a href="https://github.com/conda/conda/pull/1727"&gt;bug&lt;/a&gt; reports on the &lt;code&gt;conda&lt;/code&gt; github, but still no final solution. So &lt;code&gt;conda&lt;/code&gt; is
out for now.&lt;/p&gt;</summary><category term="python"></category><category term="pip"></category><category term="conda"></category></entry><entry><title>Desktop Files on Debian/Ubuntu and XDG</title><link href="http://mfactor.sdf.org/desktop-files.html" rel="alternate"></link><updated>2015-11-23T21:57:00-02:00</updated><author><name>Ivan Marin</name></author><id>tag:mfactor.sdf.org,2015-11-23:desktop-files.html</id><summary type="html">&lt;p&gt;I've been struggling for a while now with custom launchers. First, a bit of organizatoin on how the launchers are supposed to work.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;desktop file&lt;/code&gt; is a file that describes a launcher for an application. This file will be indexed by a Window Manager, like KDE or XFCE and can be shown on the menus and etc. The &lt;a href="http://standards.freedesktop.org/desktop-entry-spec/latest/apa.html"&gt;standard form&lt;/a&gt; of a desktop file is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[Desktop Entry]&lt;/span&gt;
&lt;span class="na"&gt;Version&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;1.0&lt;/span&gt;
&lt;span class="na"&gt;Type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;Application&lt;/span&gt;
&lt;span class="na"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;Foo Viewer&lt;/span&gt;
&lt;span class="na"&gt;Comment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;The best viewer for Foo objects available!&lt;/span&gt;
&lt;span class="na"&gt;Exec&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;fooview %F&lt;/span&gt;
&lt;span class="na"&gt;Icon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;fooview&lt;/span&gt;
&lt;span class="na"&gt;Actions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;Gallery;Create;&lt;/span&gt;
&lt;span class="na"&gt;Categories&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;Network&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This file should be put on a standard path in the system, like &lt;code&gt;~/.local/share/applications&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;And how to check if the file you wrote is valid? Just run &lt;code&gt;desktop-file-validate&lt;/code&gt; and it will point any errors that you have. &lt;/p&gt;
&lt;h2&gt;KDE blunder&lt;/h2&gt;
&lt;p&gt;After figuring it out how was the correct format for the file, my menus on KDE (and all actions, like changing the default application)was not changing anything. After searching for a long time, I found a solution: the context menu in Dolphin, to change the default application, was poiting to &lt;code&gt;.local/share/applications/mineapps.list&lt;/code&gt; and KDE 5 was reading the file on &lt;code&gt;.config/mimeapps.list&lt;/code&gt;. The solution? Exporting the default directory where KDE can find its icons. On my system, I changed .zshenv and added&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;XDG_DATA_DIRS=&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;XDG_DATA_DIRS&lt;/span&gt;&lt;span class="x"&gt;:/home/ispmarin/.local/share/applications&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;done!&lt;/p&gt;</summary><category term="linux"></category><category term="desktop"></category></entry><entry><title>How to learn Physics (and Mathematics) with History</title><link href="http://mfactor.sdf.org/physics-and-history.html" rel="alternate"></link><updated>2015-08-19T19:30:00-03:00</updated><author><name>Ivan Marin</name></author><id>tag:mfactor.sdf.org,2015-08-19:physics-and-history.html</id><summary type="html">&lt;p&gt;Watching the new Cosmos series about Faraday I was struck by a very simple thought: seeing how things evolved made simpler for me to understand physical concepts. I don’t know if I had that epiphany before or if that’s common, but for me it was a revelation. Very simple things, like magnetic fields, were way more clear after showing what Faraday was trying to achieve and was thinking than any Electromagnetism A that I did as an undergrad. And I believe that this might help my new endeavor: understanding probability deeply.&lt;/p&gt;
&lt;p&gt;I’ve been reading Lady Luck, by Warren Weaver, and I’m loving it. It has a very unique balance between telling the history of probability with showing the basic concepts in a very clear way. Another fantastic thing about this book is that it doesn’t save on words: Warren repeats and repeats what it is and what is not, so it’s perfectly clear on every moment what he’s talking about. That’s uncommon for a math book: usually the authors assume that everyone knows what he means and that a beginner easily can understand everything that is going on when a formula or a concept is demonstrated. The terror of physicists – “it can be easily demonstrated” is avoided at all costs. The trade off is that sometimes verbosity can be a little tiresome, but for me it has a clarity that I wished all my undergrad textbooks had. And that’s the connection between the Cosmos episode and this book: both show the concepts from a basic point of view, but they excel in going deep in the subject without being condescending or jumping “obvious” steps, showing the history behind that idea and how it came to be.&lt;/p&gt;
&lt;p&gt;At least on my first years as a Physics undergrad, all students were eager to experiment, to go hands on the physics labs or the observatory. Two things were hammered down on us, one good and one bad. The good one was that we had to have method: just poking things randomly would not take us very far, as fun as it was. Teaching this method (called, despite all controversy, the scientific method) was one of the best things that I could have learned as an undergrad. The bad thing was that we were buried in math courses without any kind of context. On the first year alone we had an ordinary and partial differential equations course, a analytical geometry course, two calculus courses and a basic programming course, plus some optional advanced physics math courses. I know that this is common on physics courses, but those were just dumped on us. Some physics teachers tried to give us context, but it was more on the ping-pong version (“this you will see on your calculus course”, “this you will see on your physics course” and so on) and not very helpful. This demotivated me and several other students. Why did we need to learn all this calculus and geometry without any tangible reason? (The “you’re going to need this to understand that” is not very helpful, at all. You don’t know that yet!). Only after my first classical mechanics course things started to make more sense, and got better with a great electromagnetism teacher that showed how things were put together by Maxwell and Faraday.&lt;/p&gt;
&lt;p&gt;Maybe this is about the “math first”or “physics first” dilemma for a Physics course. If you already know the math you can concentrate on the Physics ideas and results. If you know the physics principles before, the math is the best support for those ideas to flourish and be useful. (On an aside, I don’t agree with a teacher that I had that said “all you need is the physical intuition, the math is the easy part! Follow your intuition!” and every time we needed the math, he would fail.) I can see both sides now and would propose a third option: teach the history first. With the context things would be easier on both sides, the ideas would permeate the ideas on both sides and maybe provide more support for thought and understanding. This maybe would prevent the effect of trusting only on the math, without reflecting what the math is telling you. As another great teacher said, “What is a sign? It’s the difference of sending the astronaut to the moon or killing him!”&lt;/p&gt;</summary><category term="physics"></category><category term="mathematics"></category><category term="history"></category><category term="learning"></category></entry><entry><title>A history of packages and libraries or: how Spark and Hadoop are doing the same mistake as BLAS and LAPACK</title><link href="http://mfactor.sdf.org/a-history-of-packages-and-libraries-or-how-spark-and-hadoop-are-doing-the-same-mistake-as-blas-and-lapack.html" rel="alternate"></link><updated>2015-07-20T09:33:00-03:00</updated><author><name>I M</name></author><id>tag:mfactor.sdf.org,2015-07-20:a-history-of-packages-and-libraries-or-how-spark-and-hadoop-are-doing-the-same-mistake-as-blas-and-lapack.html</id><summary type="html">&lt;p&gt;A long time ago, on a land of big iron and MPI, there were some
libraries that were used everywhere. The libraries were fundamental to
most of the important computation that was done, they were open source
and all the cool kids on the block knew them. They also rarely were
distributed by the Linux distributions, and several versions needed to
coexist to support all the high performance software that needed to be
run. They needed to be compiled, and different versions of compilers and
libraries and binaries kept spiralling out of control. Keeping them
working was already a huge task, and working well was even harder. And
yes, I'm talking about BLAS and LAPACK.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.netlib.org/blas/"&gt;BLAS&lt;/a&gt; (Basic Linear Algebra Suprograms)
and &lt;a class="reference external" href="http://www.netlib.org/lapack/"&gt;LAPACK&lt;/a&gt; (Linear Algebra PACKAge),
from &lt;a class="reference external" href="http://www.netlib.org/"&gt;Netlib&lt;/a&gt;, were fundamental on the HPC
era as the backbone of inumerous projects. The stack on those days had
one flavour or another of
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface"&gt;MPI&lt;/a&gt; to run
the parallel jobs in huge clusters, together with
&lt;a class="reference external" href="http://www.netlib.org/scalapack/"&gt;ScaLAPACK&lt;/a&gt;. And the job of keeping
these libraries working with several different compilers (mainly
&lt;a class="reference external" href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt;,
&lt;a class="reference external" href="https://software.intel.com/en-us/intel-compilers"&gt;Intel&lt;/a&gt; and
&lt;a class="reference external" href="http://www.pgroup.com/"&gt;Portland Group&lt;/a&gt; compilers ) and linking
binaries objects were the nightmare of any sysadmin. Written mainly in
Fortran, with wrappers for C and C++, a sysadmin had to keep the entire
stack compiled with the right version of compilers and linked with the
right libraries. Projects like &lt;a class="reference external" href="http://www.netlib.org/atlas/"&gt;ATLAS&lt;/a&gt;
tried to help a bit on the side of performance optimization, while
vendors had their own binary distribution, tuned to their processors
(&lt;a class="reference external" href="https://software.intel.com/en-us/intel-mkl"&gt;MKL&lt;/a&gt; for Intel and
&lt;a class="reference external" href="http://developer.amd.com/tools-and-sdks/cpu-development/amd-core-math-library-acml/"&gt;ACML&lt;/a&gt;
for AMD).&lt;/p&gt;
&lt;p&gt;A sysadmin on the HPC world had to know all these libraries, how to
deploy them to the users that wanted to compile their own programs, and
how to make all of them play nice with each other on the same cluster.
It was hard to keep track of what was breaking, with lots of non
standard paths and binary objects, name mangling, and users trying to do
things that you never imagined possible. The HPC sysadmin also needed to
know the underlying architecture very well, including the network and
storage structures, the physical layout of the cables, so he or she
could have a chance to know what was breaking or lagging.&amp;nbsp; Slowly, these
tools started to be better organized, with practices like module loading
and the linux distributions creating packages to the basic libraries,
even if the performance was not the best possible. And then the Cloud
happened.&lt;/p&gt;
&lt;p&gt;(I'm not going to dwelve on the HPC vs Cloud debate, not even try to
touch the Big Data vs BI vs computing debates, and try to focus on the
tools that make all these acronyms possible, otherwise this would be a
very long post.)&lt;/p&gt;
&lt;p&gt;I have some time now on my hands and decided to check how the deployment
and configuration of a Spark cluster is being done on
&lt;a class="reference external" href="https://spark.apache.org/docs/latest/"&gt;Debian&lt;/a&gt;. And boy, &lt;a class="reference external" href="http://spark.apache.org/docs/latest/building-spark.html"&gt;what a
surprise&lt;/a&gt;.
I've been seeing several posts from sysadmins complaining about the
dockerization of applications and the mirage of the devops, but was
taken aback with the state of how Spark is suposed to be deployed on a
Debian (&lt;a class="reference external" href="http://bigtop.apache.org/book/apache-bigtop-user-guide/apache-bigtop-user-guide.html"&gt;or any other
linux&lt;/a&gt;)
installation. There are several binaries spread out, with
interdependencies, depending on the compiler version or library version,
without standard paths, and users making a even larger mess... where did
I hear about it before?&lt;/p&gt;
&lt;p&gt;I expected that with the amout of resources that are being poured on the
cloud projects, some form of organization or standardization would have
happened. But no luck: all is still downloading binaries and compiling
tons of disconnected but interdependent libraries, only with more levels
of abstraction. Yes, the abstraction levels help the user, as he doesn't
need anymore to know about if the processor has SSE3 or SSE4.2. Yes, now
devops can be both programmers and sysadmins. Yes, you don't have to buy
new machines, install them, plug the cables - you can just go to the AWS
console and click moar instances. But someone, on the end, has to move
all this to production, keep it working, plan on how this will grow, and
fix it when it breaks - and it will break, trust me.&lt;/p&gt;
&lt;p&gt;Oh, but there's &lt;a class="reference external" href="https://www.openstack.org/"&gt;OpenStack&lt;/a&gt;to the
rescue! Well, not really. I'm also not going to go deep into OpenStack,
but suffice to say that OpenStack is more an orchestration service than
what I'm looking into here.&lt;/p&gt;
&lt;p&gt;And oh, there's Docker/Vagrant/CoreOS/etc containers! Well, that's also
an issue that I think is not the focus of this discussion. During the
HPC era, usually the application deployment consisted of: configuring
the computing nodes to communicate, share the data using a dedicated
network connected to a RAID server and running a binary on a NFS share.
Things got way more complicated because they are more complicated:
connection to several databases, web services, what not. But in the end
the problem is the same: how to deploy an environment that you can
maintain and fix? These technologies help immensely on empowering the
developer to create more complex solutions and abstract more, especially
parallel computations, but they actually make the maitain and fix part
way more complicated, from the sysadmin perspective. That's why I'm not
trying to address again these tools: they solve another problem, and
anyway, you have to configure these containers with the same binaries
and libraries, that need to be compiled, going back to the initial
problem!&lt;/p&gt;
&lt;p&gt;So, is there a way to solve this mess? Unfortunately, I don't know. I've
been sidelining all these issues on the day to day at work, adding
binaries and virtualenvs when needed, but this will explode soon or
later. I just think that this problem is not being looked with much
care, especially on the startup world, or just being pushed to the new
&lt;a class="reference external" href="https://anotherlifeform.files.wordpress.com/2015/07/7d267-11359006_420191058182770_2015412754_n.jpg"&gt;shiny new
thing&lt;/a&gt;that
will solve everything with more abstraction. What to do? Maybe to think
about these points when deploying your cluster, playing better with the
basic distributions and contributing more, and the distros trying not to
suffer from NIH syndrome would help.&lt;/p&gt;
&lt;p&gt;I'm all ears for suggestions!&lt;/p&gt;
</summary><category term="beowulf"></category><category term="big data"></category><category term="debian"></category><category term="haddop"></category><category term="openstack"></category><category term="spark"></category></entry><entry><title>Python reads on data understanding and web development</title><link href="http://mfactor.sdf.org/python-reads-on-data-understanding-and-web-development.html" rel="alternate"></link><updated>2015-07-08T04:45:00-03:00</updated><author><name>I M</name></author><id>tag:mfactor.sdf.org,2015-07-08:python-reads-on-data-understanding-and-web-development.html</id><summary type="html">&lt;p&gt;I´ve been overwhelmed by the number of good publications, workbooks and
other material on Python and Data Science on the web. I tried to keep
track and read everything, but my time on the job has been limited for
that. I decided to keep a list of the sources that I´m reading or want
to read here. They are, in no order:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.fullstackpython.com/"&gt;http://www.fullstackpython.com/&lt;/a&gt; Awesome resource for Python web
development.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://plot.ly/ipython-notebooks/survival-analysis-r-vs-python/"&gt;https://plot.ly/ipython-notebooks/survival-analysis-r-vs-python/&lt;/a&gt;
Survival analysis in R and Python. Excellent resource to combine
ipython notebooks with R. (The entire plotly notebooks collection is
awesome. I would love if plotly was open source or at least free.)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/donnemartin/data-science-ipython-notebooks"&gt;https://github.com/donnemartin/data-science-ipython-notebooks&lt;/a&gt; This is
an awesome collection of notebooks with all the parts of a good data
stack on Python. Still no time to review them all, but so far, very
good and detailed. &lt;a class="reference external" href="http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/effect_size.ipynb"&gt;This is
one&lt;/a&gt;
of the best on the statistical side.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://people.duke.edu/~ccc14/sta-663/"&gt;http://people.duke.edu/~ccc14/sta-663/&lt;/a&gt; This one is very through, even
if sometimes a bit rough on the edges. Very good reference material.
There are other materials from the folks at Duke that are very good,
like &lt;a class="reference external" href="http://people.duke.edu/~ccc14/pcfb/"&gt;this&lt;/a&gt; one, a bit more
biological.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/josephmisiti/awesome-machine-learning"&gt;https://github.com/josephmisiti/awesome-machine-learning&lt;/a&gt; More on the
line of Machine Learning, this collection is also quite good, if a
bit extensive. The objective is more to list all resources and
libraries on several languages about Machine Learning than just
Python, but nonetheless the Python section is good.&lt;/li&gt;
&lt;li&gt;Keeping the more generalist vibe,&lt;a class="reference external" href="http://www.wzchen.com/data-science-books"&gt;this
list&lt;/a&gt;of free books
about Data Science has the basic stuff for the job.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will keep things updated here as the list grows. Please send me
suggestions of good resources that you want to see on this list!&lt;/p&gt;
</summary><category term="data science"></category><category term="Python"></category><category term="web development"></category></entry><entry><title>Finally, Python 3!</title><link href="http://mfactor.sdf.org/finally-python-3.html" rel="alternate"></link><updated>2015-06-27T09:39:00-03:00</updated><author><name>Ivan Marin</name></author><id>tag:mfactor.sdf.org,2015-06-27:finally-python-3.html</id><summary type="html">&lt;p&gt;It's 2015 and it's the first time that I'm using Python 3
professionally. And I have to admit, if I knew that all my scripts were
going to work so easily with 3, I would have changed way before. There
were a few hurdles with the Python 3 version of some libraries - patsy,
I'm looking at you - but so far the transition was painless. The things
that I had to change were expected, like the print function and some
dict operations, and xrange. I know it's for the best now, so let's
roll.&lt;/p&gt;
&lt;p&gt;First of all, I switched from WinPython to Anaconda. Having to work on a
constrained Windows installation, Anaconda provides better support for
installing packages than pip alone. And as so far using virtualenv has
not been a success on Windows, I'm going to try the conda environment
for my projects.&lt;/p&gt;
&lt;p&gt;On the version control front, I finally got access to a git shell on
Windows, and now I can keep my sanity working with code. There's even a
Team Foundation Server plugin for it, and the client is not that bad -
actually, it's useful and keeps the git terminology, helping immensely
someone that was used to git on the command line. Not bad, Microsoft.
Git-cola is still better and is several times smaller than TFS, but hey,
sometimes we have to use what we got. So my working environment now is&lt;/p&gt;
&lt;p&gt;* Anaconda 2.2.0, Python 3 version&lt;/p&gt;
&lt;p&gt;* Packages:&lt;/p&gt;
&lt;p&gt;-pandas, sklearn, matplotlib, calendar, sqlalchemy with Oracle plugin,
seaborn (mostly for styles), statsmodels.&lt;/p&gt;
&lt;p&gt;* Editor:&lt;/p&gt;
&lt;p&gt;Vim and ipython notebook (depending on the type of work)&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://msysgit.github.io/"&gt;Git for Windows&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;and Tableau for some visualizations.&lt;/p&gt;
</summary></entry><entry><title>Analytics in a locked world</title><link href="http://mfactor.sdf.org/analytics-in-a-locked-world.html" rel="alternate"></link><updated>2015-04-21T19:25:00-03:00</updated><author><name>I M</name></author><id>tag:mfactor.sdf.org,2015-04-21:analytics-in-a-locked-world.html</id><summary type="html">&lt;p&gt;As a data scientist, I have to deal with data from several different
sources. This begs for a wide range of tools and great flexibility of
using these tools, otherwise manipulating the data can be almost
impossible. And then your employer says that you can only use Windows,
without admin rights, and you can only install software from a very
restrict pre-approved list. What do you do? Go to a corner and cry?&lt;/p&gt;
&lt;p&gt;Almost. But then, you find a breach, a sliver of hope. You can install
python on your machine. And in this moment, you smile, asking if you're
up to the challenge. (Dramatic, no?)&lt;/p&gt;
&lt;p&gt;With one of the best Python installations for Windows is
&lt;a class="reference external" href="https://winpython.github.io/"&gt;WinPython&lt;/a&gt;, that comes prepackaged
with &lt;a class="reference external" href="https://github.com/winpython/winpython/issues/56"&gt;Numpy, Scipy and
Pandas&lt;/a&gt; and other
neat stuff, you can start really working. The second best part is that
you can install on your user folder, so you don't need admin rights.
Ufta! And the savior for sane workflows in this setting is
&lt;a class="reference external" href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt;, the new IPython shell. It is awesome
for the data and modelling workflow.&lt;/p&gt;
&lt;p&gt;My workflow now consists of:&lt;/p&gt;
&lt;p&gt;1. Getting all raw data in one place, be excel spreadsheets (don't ask),
CSV files, word files (I said don't ask!), and some data banks. Most of
my data are time series.&lt;/p&gt;
&lt;p&gt;2. Importing the data or connecting the services on a jupyter python
notebook. I have to connect to an oracle BD, so
&lt;a class="reference external" href="http://cx-oracle.sourceforge.net/"&gt;cx_Oracle&lt;/a&gt; is the connector.&lt;/p&gt;
&lt;p&gt;3. Parsing the data with
&lt;a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.ExcelFile.parse.html"&gt;ExcelFile&lt;/a&gt;
from &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;. It can be tricky. As each
sheet is different, I use a JSON file with the starting line, starting
and ending columns, number of lines to be read and headers. That way I
can process it automatically, and if the file changes is quite easy to
adjust.&lt;/p&gt;
&lt;p&gt;4. Ok, clear the data! This is usually the most problematic part. One of
the things that I had to do was to convert zero values to NaNs. The
reason is that zero is a valid value on my data, but not a real one, so
it would skew all statistics. Pandas can handle NaNs in the&lt;a class="reference external" href="http://pandas.pydata.org/pandas-docs/version/0.16.0/gotchas.html"&gt;Right
Way&lt;/a&gt;,
so you would be better off. But beware, there are a few problems with
&lt;a class="reference external" href="http://stackoverflow.com/questions/29747850/error-using-bootstrap-plot-in-pandas-if-values-have-nan"&gt;some
functions&lt;/a&gt;,
so you have to be careful anyway.&lt;/p&gt;
&lt;p&gt;5. With the data in a proper format, (&lt;a class="reference external" href="www.jstatsoft.org/v59/i10/paper"&gt;tidy
format&lt;/a&gt;, NaNs for invalid values,
all values with proper references), I start doing some exploratory
analysis on the data. For now, basic stuff like line graphs, boxplots,
averages by year and month, autocorrelation and some rolling means, and
scatter plots.&lt;/p&gt;
&lt;p&gt;6. With that things start to get more interesting. I decided to use
&lt;a class="reference external" href="http://scikit-learn.org/stable/"&gt;Scikit-Learn&lt;/a&gt; to do the regressions
instead of statsmodels because of the next step, prediction. You have to
transform the data again so scikit can understand it, but it's&lt;a class="reference external" href="http://stackoverflow.com/questions/29748717/use-scikit-learn-to-do-linear-regression-on-a-time-series-pandas-data-frame"&gt;not
that
hard&lt;/a&gt;,
and worth the while. Now I do some stardard Linear Squares to get the
trend and a RANSAC to get more specific results.&lt;/p&gt;
&lt;p&gt;And all this inside a ipython notebook! Pretty awesome, right? Next
steps, do a full
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins"&gt;Box-Jenkins&lt;/a&gt; on
the results.&lt;/p&gt;
</summary><category term="pandas"></category><category term="Python"></category></entry><entry><title>Development with Python on Linux is turning into Windows</title><link href="http://mfactor.sdf.org/development-with-python-on-linux-is-turning-into-windows.html" rel="alternate"></link><updated>2015-03-15T06:05:00-03:00</updated><author><name>ispmarin</name></author><id>tag:mfactor.sdf.org,2015-03-15:development-with-python-on-linux-is-turning-into-windows.html</id><summary type="html">&lt;p&gt;Yes, the title is a flamebait, but please bear with me.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image0" src="https://dxprience.files.wordpress.com/2014/08/bear-with-me.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;As I said in the last post, I installed Debian on my Chromebook using
&lt;a class="reference external" href="https://github.com/dnschneid/crouton"&gt;crouton&lt;/a&gt;. It worked quite
easily, KDE installed. But then I started building my dev environment,
using Python. But first, a quick digression.&lt;/p&gt;
&lt;p&gt;I have to use Windows at work now. I know, I know, but there's no way
around it. Setting up a dev environment on Windows is very very tricky
and not very funny, especially if you don't have admin rights on your
own machine. I managed to get the basics working, but it was not easy.
The loop search internet-&amp;gt;download-&amp;gt;install-&amp;gt;internet was not fun at all
after all these years with apt-get.&lt;/p&gt;
&lt;p&gt;So, when I started setting up my environment on the Debian on the
chromebook it struck me. &lt;em&gt;I'm doing exactly as I done in Windows.&lt;/em&gt; How
so? First, I installed Python 2.7.9 via apt-get. Nothing funny so far.
Then I decided to use a virtualenv. Ops. To install it I can use the
Debian package or download a newer version directly. Ok, ok... but then
it comes virtualenvwrapper. Ok, I still can install it using the Debian
repos, but it's an old version. After I set up my virtualenv, I start
using pip. Pip is ok, but now I have to download all packages from
different places. I know that this is one of the features of pip, but
nonetheless is divorced from the operating system, there is no search to
speak about inside pip, there is no update all, etc etc. Ok, but there
is still hope, right? Next thing, PyCharm. Bam, gotta go back to the
internet, download it, install... Windows again!&lt;/p&gt;
&lt;p&gt;You can argue that I chose to use these packages, that there are several
in the Debian repos that can satisfy my needs. That this is just a rant
that &amp;quot;package A or B is not in the repos!&amp;quot;, and I tend to agree with
you. But I didn't have to do nothing like this when I was developing in
C, C++ or Fortran, including libraries, so I started thinking that maybe
the Python tools integration with Debian or any other Linux distro is
not as good as it could be. What can we do to make it easier? So far I
don't know. Any suggestions?&lt;/p&gt;
</summary><category term="debian"></category><category term="pip"></category><category term="PyCharm"></category><category term="Python"></category></entry><entry><title>Using Virtual Machines and Ubuntu to Keep a Sane Development Environment</title><link href="http://mfactor.sdf.org/using-virtual-machines-and-ubuntu-to-keep-a-sane-development-environment.html" rel="alternate"></link><updated>2012-12-11T15:06:00-02:00</updated><author><name>ispmarin</name></author><id>tag:mfactor.sdf.org,2012-12-11:using-virtual-machines-and-ubuntu-to-keep-a-sane-development-environment.html</id><summary type="html">&lt;p&gt;I tried. I swear I tried. But it's plain and simple painful to develop
in Fortran in Windows. There, I said it. Is it &lt;em&gt;possible&lt;/em&gt;? Yes. Is it
easy? Not a chance in Hell.&lt;/p&gt;
&lt;p&gt;First of all, there aren't a lot of options for Fortran Compilers for
the Microsoft Windows Platform (and I'm talking about Windows 7, I would
imagine that Windows 8 would be worse.). The most sane one, Intel
Fortran, is even now Intel Visual Fortran Compiler for Windows (&lt;a class="reference external" href="http://software.intel.com/en-us/forums/intel-visual-fortran-compiler-for-windows"&gt;I kid
you
not&lt;/a&gt;.).
And then there's the interfaces. Microsoft provides for free the
&lt;a class="reference external" href="http://www.microsoft.com/visualstudio/eng/products/visual-studio-express-products"&gt;Express&lt;/a&gt;
suit, so things are not that bad, right? No. The Express suit can't use
Intel Visual Fortran Compiler for Windows support, so you're out of luck
unless you buy the full Visual Studio Suite, and which one I dunno -
never understood the Ultimate/Professional/Premium/Alien/KitchenCleaning
version confusion. Eclipse CDT/Photran would be a good one instead,
right? No. Intel Visual Fortran Compiler for Windows also doesn't
support Eclipse CDT/Photran. Where to run?&lt;/p&gt;
&lt;p&gt;Maybe MinGW could help us? Yes, maybe. Both Intel Visual Fortran
Compiler for Windows and MinGW can compile a Fortran source file from
the command line, but we're in friggin Windows, right? Also, for large
projects, code completion and syntax errors are a must. I didn't try the
LLVM/dragonegg and mingw-fortran combination, but as I wouldn't get the
advantages that I'm looking for, I decided that if I have to use a
Windows machine, no one would prevent to use a sane, decent Linux
environment &lt;em&gt;inside&lt;/em&gt; that Windows machine. Ubuntu plus VirtualBox it is!&lt;/p&gt;
&lt;p&gt;Downloaded 12.10-amd64, installed in a VDI image, all with standard
options. Of course Ubuntu would give me some headaches: had to uninstall
that creepy shopping lens, install Google Chrome. Google Chrome
installation was interesting: the Software Center gave me an &amp;quot;This
package is of low quality. Don't install it.&amp;quot; Just hit ignore and
install it anyway. And Ubuntu froze. Unity recomposed itself, but I
figured that there is something funky going on between X and VirtualBox,
so I went back to VirtualBox settings and disabled the 3D video, what
made things a bit better - but still no changing resolution with
changing the size of the VM window. Strange. Will probably install KDE
anyway... so I plugged my external drive to copy all the files to the
VM. And then, Compiz hanged again. Definitely annoying. So while I
waited to see what the 12.10 would do (keep freezed, it seems), decided
to use my 12GB of RAM and my Core i7 and hit an 12.04 installation. It
started to be a little ironic that I was running away from one platform
that is impossible to program to another one that refuses to work to
begin with. I decided to keep going, nonetheless, and see where I would
get.&lt;/p&gt;
&lt;p&gt;I still kept my hopes that things would be easier after I got an Ubuntu
running. My plan was to use the VM as a full development environment
that I could transport easily to different computers. I thought about
keeping a git repository of the important stuff, but the multiplatform
problems that I detailed above with Fortran made this a no-go. 12.04 was
much better than 12.10, and while 12.10 was still frozen, I managed to
install 12.04, update it, install the required dkms modules for
VirtualBox. Also downloaded Eclipse CDT, &lt;a class="reference external" href="http://eclipsecolorthemes.org/"&gt;Eclipse Colour
Plugin&lt;/a&gt;,
&lt;a class="reference external" href="http://pydev.org/"&gt;Pydev&lt;/a&gt; and
&lt;a class="reference external" href="http://www.eclipse.org/photran/"&gt;Photran&lt;/a&gt;,
&lt;a class="reference external" href="http://openjdk.java.net/"&gt;OpenJDK&lt;/a&gt;, everything running ok! Copied
from my external drive all the source files, all beautiful. Decided not
to use 12.10 and stick with 12.04. So now, the next step: moving the VM
to the external drive.&lt;/p&gt;
&lt;p&gt;After copying the entire VB folder to the external drive, I would have
to test it and see if there's anything missing. And it works! Now doing
some backups on external hard drives and transporting happily my virtual
machines around. It's cumbersome to transport 20GB up and down an
external drive every time, but it's better than use Windows to program,
that's for sure!&lt;/p&gt;
&lt;p&gt;One last trick: to share a folder between the Ubuntu guest and Windows
host and be able to create soft symlinks, I had to issue this command in
the Windows host:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
VBoxManage setextradata VM_NAME VBoxInternal2/SharedFoldersEnableSymlinksCreate/SHARE_NAME 1


Where VM_NAME is your virtual machine name. Done!
&lt;/pre&gt;
</summary><category term="fortran"></category><category term="symlink"></category><category term="ubuntu"></category><category term="virtualbox"></category><category term="windows"></category></entry></feed>